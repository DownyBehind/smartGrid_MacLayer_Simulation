#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OMNeT++ 6.1 / opp_scavetool ê²°ê³¼ í›„ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- í•„í„° ì—†ì´ ì „ì²´ ë¤í”„(CSV-R, CSV-S) â†’ Pythonì—ì„œ name/moduleë¡œ í•„í„°ë§
"""

import os
import sys
import glob
import csv
import io
import subprocess
import pandas as pd
from typing import List, Tuple

# EDCA AC ì¸ë±ìŠ¤ â†’ ì´ë¦„ ë§¤í•‘
AC_MAP = {0: 'AC_VO', 1: 'AC_VI', 2: 'AC_BE', 3: 'AC_BK'}

# ------------------------- ì‚¬ìš©ì ì„¤ì • -------------------------
SCRIPT_DIR   = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR   = os.path.join(SCRIPT_DIR, 'data')
RESULTS_DIR  = os.path.join(SCRIPT_DIR, '..', '..', 'ini', 'results')

# ë²¡í„° í•„í„° í‚¤ì›Œë“œ(ë¶€ë¶„ ë¬¸ìì—´ AND)
THR_NAME_KEYS    = ['packetReceivedFromPeer']   # with/withoutRetry ëª¨ë‘ ë§¤ì¹˜
THR_MODULE_KEYS  = []                           # ëª¨ë“ˆ ì œí•œ ì—†ìŒ
CW_NAME_KEYS     = ['contentionWindowChanged']  # EDCA/DCF ê³µí†µ í‚¤ì›Œë“œ

# ìŠ¤ì¹¼ë¼ ì´ë¦„(ì •í™• ì¼ì¹˜)
RETRY_NAMES = [
    'packetSentToPeerWithRetry:count',
    'packetSentToPeerWithoutRetry:count',
    # í•„ìš” ì‹œ ë°”ì´íŠ¸ í•©ê³„ë„ í™•ì¸ ê°€ëŠ¥
    # 'packetSentToPeerWithRetry:sum(packetBytes)',
    # 'packetSentToPeerWithoutRetry:sum(packetBytes)',
]

# âœ… DCF ì „ìš© ì§€í‘œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# SENT_NAME / SENT_MODULE_KEY ì œê±°
# DROP_NAME / DROP_MODULE_KEYë„ ëª¨ë“ˆ ì œí•œì„ ë‘ì§€ ì•ŠìŠµë‹ˆë‹¤.
DROP_NAME = 'retryLimitReached'   # INET 4.5.4 í˜¸í™˜
# ---------------------------------------------------------------


def run_cmd(cmd: List[str], desc: str) -> str:
    print(f"ğŸš€ {desc}...")
    print("   $", " ".join(cmd))
    try:
        p = subprocess.run(cmd, check=True, text=True,
                           capture_output=True, encoding='utf-8')
        if p.stdout.strip():
            print(p.stdout.strip())
        print("âœ… ì™„ë£Œ\n")
        return p.stdout
    except FileNotFoundError:
        print(f"âŒ '{cmd[0]}' ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PATH í™•ì¸.")
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"âŒ ì—ëŸ¬: {desc}")
        print("---- stdout ----")
        print(e.stdout)
        print("---- stderr ----")
        print(e.stderr)
        sys.exit(1)


def ensure_files() -> Tuple[List[str], List[str]]:
    vec_files = glob.glob(os.path.join(RESULTS_DIR, '*.vec'))
    sca_files = glob.glob(os.path.join(RESULTS_DIR, '*.sca'))
    if not vec_files and not sca_files:
        print(f"âŒ .vec/.sca ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {RESULTS_DIR}")
        sys.exit(1)
    print(f"ğŸ” vec files: {len(vec_files)}ê°œ / sca files: {len(sca_files)}ê°œ ë°œê²¬")
    return vec_files, sca_files


def dump_all(vec_files: List[str], sca_files: List[str],
             vec_out: str, sca_out: str):
    # ë²¡í„° ì „ì²´
    cmd_v = ['opp_scavetool', 'x', '-v', '-F', 'CSV-R', '-o', vec_out] + vec_files
    run_cmd(cmd_v, "ë²¡í„° ì „ì²´ ë¤í”„")

    # ìŠ¤ì¹¼ë¼+íŒŒë¼ë¯¸í„° ì „ì²´
    cmd_s = ['opp_scavetool', 'x', '-F', 'CSV-S', '-x', 'allowMixed=true', '-o', sca_out] + sca_files
    run_cmd(cmd_s, "ìŠ¤ì¹¼ë¼/íŒŒë¼ë¯¸í„° ì „ì²´ ë¤í”„")


def read_scavetool_csv(path: str) -> pd.DataFrame:
    """opp_scavetool CSVì—ì„œ í—¤ë” ì¤„ì„ ì°¾ì•„ pandasë¡œ ì½ëŠ”ë‹¤"""
    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()
    header_idx = None
    for i, l in enumerate(lines):
        if l.strip().startswith('run,') and ',module,' in l and ',name,' in l:
            header_idx = i
            break
    if header_idx is None:
        raise ValueError(f"Header not found in {path}")
    csv_text = ''.join(lines[header_idx:])
    return pd.read_csv(io.StringIO(csv_text), engine='python', on_bad_lines='skip')

def filter_vectors(in_csv: str, out_csv: str,
                   name_keys: List[str] = None,
                   module_keys: List[str] = None):
    name_keys   = name_keys or []
    module_keys = module_keys or []

    df = read_scavetool_csv(in_csv)

    # í—¤ë” ì»¨í…ìŠ¤íŠ¸ ì „íŒŒ
    for col in ['run', 'module', 'name']:
        if col in df.columns:
            df[col] = df[col].replace('', pd.NA).ffill()

    # CSV-R ë²¡í„° ì»¬ëŸ¼ í™•ì¸
    if 'vectime' in df.columns and 'vecvalue' in df.columns:
        tcol, vcol = 'vectime', 'vecvalue'
        # âœ… ìˆ«ì ë³€í™˜ ê¸ˆì§€! ë¬¸ìì—´ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ë§Œ í™•ì¸
        sel = df[tcol].astype(str).str.strip().ne('') & df[vcol].astype(str).str.strip().ne('')
    else:
        # (CSV â€˜ìƒ˜í”Œë‹¹ 1í–‰â€™ í˜•ì‹ì¼ ë•Œë§Œ ìˆ«ì í™•ì¸)
        tcol, vcol = 'time', 'value'
        sel = pd.to_numeric(df[tcol], errors='coerce').notna() & pd.to_numeric(df[vcol], errors='coerce').notna()

    # ì´ë¦„/ëª¨ë“ˆ í‚¤ì›Œë“œ(AND)
    for k in name_keys:
        sel &= df['name'].str.contains(k, case=False, na=False)
    for k in module_keys:
        sel &= df['module'].str.contains(k, case=False, na=False)

    kept = df.loc[sel].copy()
    kept.to_csv(out_csv, index=False)
    print(f"â„¹ï¸  {os.path.basename(out_csv)}: {len(kept)} vector data rows kept "
          f"(name_keys={name_keys}, module_keys={module_keys})")


def filter_rows(in_csv: str, out_csv: str,
                name_keys: List[str] = None,
                module_keys: List[str] = None,
                name_exact: List[str] = None):
    """ë¶€ë¶„/ì •í™• ì¼ì¹˜ë¡œ í–‰ í•„í„°ë§"""
    name_keys   = name_keys or []
    module_keys = module_keys or []
    name_exact  = name_exact or []

    df = read_scavetool_csv(in_csv)

    sel = pd.Series([True] * len(df))
    if name_exact:
        sel &= df['name'].isin(name_exact)

    if name_keys:
        for k in name_keys:
            sel &= df['name'].str.contains(k, case=False, na=False)

    if module_keys:
        for k in module_keys:
            sel &= df['module'].str.contains(k, case=False, na=False)

    kept_df = df[sel].copy()
    kept_df.to_csv(out_csv, index=False)
    print(f"â„¹ï¸  {os.path.basename(out_csv)}: {len(kept_df)} rows kept "
          f"(name_exact={name_exact}, name_keys={name_keys}, module_keys={module_keys})")


def to_float_safe(x):
    try:
        return float(x)
    except Exception:
        return None


def sum_scalar(in_csv: str, target_name: str, module_key: str = None) -> float:
    df = read_scavetool_csv(in_csv)
    df['value_num'] = df['value'].apply(to_float_safe)
    sel = df['name'] == target_name
    if module_key:
        sel &= df['module'].str.contains(module_key, case=False, na=False)
    return df.loc[sel, 'value_num'].sum(skipna=True)

def _parse_series(cell: str):
    if not isinstance(cell, str):
        return []
    s = cell.strip().replace('[',' ').replace(']',' ').replace(',',' ')
    parts = [p for p in s.split() if p]
    try:
        return [float(p) for p in parts]
    except ValueError:
        return []

def compute_avg_bps(throughput_csv_path, out_csv_path):
    df = read_scavetool_csv(throughput_csv_path)
    if df.empty:
        print("ğŸ“ˆ ìŠ¤ë£¨í’‹ ê³„ì‚°: ì…ë ¥ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    rows = []
    if 'vectime' in df.columns and 'vecvalue' in df.columns:
        # âœ… CSV-R: í•œ í–‰ì— ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŒ
        for _, r in df.iterrows():
            times = _parse_series(r.get('vectime'))
            vals  = _parse_series(r.get('vecvalue'))
            if len(times) >= 2 and len(vals) == len(times):
                duration = times[-1] - times[0]
                totalB   = sum(vals)
                if duration > 0:
                    rows.append({
                        'run': r['run'], 'module': r['module'], 'name': r['name'],
                        'samples': len(times), 'duration': duration,
                        'sum_bytes': totalB, 'avg_bps': 8.0 * totalB / duration
                    })
    else:
        # (CSV â€˜ìƒ˜í”Œë‹¹ 1í–‰â€™ í˜•ì‹ì¼ ë•Œ)
        tcol = 'time'; vcol = 'value'
        df[tcol] = pd.to_numeric(df[tcol], errors='coerce')
        df[vcol] = pd.to_numeric(df[vcol], errors='coerce')
        df = df.dropna(subset=[tcol, vcol])
        for (r, m, n), g in df.groupby(['run','module','name']):
            g = g.sort_values(tcol)
            if len(g) < 2:
                continue
            duration = g[tcol].iloc[-1] - g[tcol].iloc[0]
            totalB   = g[vcol].sum()
            if duration > 0:
                rows.append({'run': r, 'module': m, 'name': n,
                             'samples': len(g), 'duration': duration,
                             'sum_bytes': totalB, 'avg_bps': 8.0 * totalB / duration})

    out = pd.DataFrame(rows)
    out.to_csv(out_csv_path, index=False)
    if not out.empty:
        print(f"ğŸ“ˆ í‰ê·  ìŠ¤ë£¨í’‹ rows: {len(out)}  (ì €ì¥: {out_csv_path})")
        print(f"   ì „ì²´ í‰ê· : {out['avg_bps'].mean():.2f} bps")
    else:
        print("ğŸ“ˆ ìŠ¤ë£¨í’‹ ê³„ì‚°í•  í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
    return out


def compute_retry_ratio(all_sca_csv_path):
    """ì¬ì‹œë„ í”„ë ˆì„ ë¹„ìœ¨(%) ê³„ì‚°"""
    df = read_scavetool_csv(all_sca_csv_path)
    w  = df.loc[df['name']=='packetSentToPeerWithRetry:count','value'].astype(float).sum()
    wo = df.loc[df['name']=='packetSentToPeerWithoutRetry:count','value'].astype(float).sum()
    tot = w + wo
    ratio = (w / tot * 100.0) if tot > 0 else 0.0
    print(f"ğŸ” ì¬ì‹œë„ í”„ë ˆì„: {w:.0f}/{tot:.0f}  ({ratio:.2f}%)")
    return ratio

def cw_change_summary(cw_csv_path):
    """CW ë³€ê²½ ì´ë²¤íŠ¸ ê°œìˆ˜(ë¹ ë¥¸ sanity ì²´í¬)"""
    df = read_scavetool_csv(cw_csv_path)
    print(f"âš™ï¸ CW ë³€ê²½ ì´ë²¤íŠ¸ ìˆ˜: {len(df)}")
    return len(df)

def scan_drops(all_sca_csv_path):
    df = read_scavetool_csv(all_sca_csv_path)
    cand = df[df['name'].astype(str).str.contains('drop|queue|overflow|discard|fail', case=False, na=False)].copy()
    cand['value_num'] = pd.to_numeric(cand['value'], errors='coerce')

    # í•©ê³„ TOP10 ë¨¼ì €
    top = (cand.groupby('name')['value_num']
               .sum()
               .sort_values(ascending=False)
               .head(10))
    print("\nğŸ” Drop-like ì§€í‘œ TOP10(í•©ê³„):")
    print(top.to_string())

    # ì˜ˆì‹œ 20í–‰ ë¯¸ë¦¬ë³´ê¸°
    print("\nğŸ” ì˜ˆì‹œ 20í–‰ ë¯¸ë¦¬ë³´ê¸°:")
    print(cand[['module','name','value']].head(20).to_string(index=False))
    return cand


# ---- postProcessing.py (ì¼ë¶€) ----
def total_tx_frames(all_sca_csv_path: str) -> float:
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = df['value'].apply(to_float_safe)
    # EDCAÂ·DCF ê³µí†µìœ¼ë¡œ ì´ë¦„ë§Œ ë³´ê³  í•©ê³„
    return df[df['name'].isin(RETRY_NAMES)]['value_num'].sum(skipna=True)



def total_retrylimit_drops(all_sca_csv_path: str) -> float:
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = df['value'].apply(to_float_safe)
    mask = (df['name'] == 'packetDropRetryLimitReached:count')
    return df.loc[mask, 'value_num'].sum(skipna=True)

def total_arp_fail(all_sca_csv_path: str) -> float:
    """IP ê³„ì¸µì˜ ì£¼ì†Œí•´ê²° ì‹¤íŒ¨(ARP ì‹¤íŒ¨) ì´í•©."""
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = df['value'].apply(to_float_safe)
    mask = (df['name'] == 'packetDropAddressResolutionFailed:count')
    return df.loc[mask, 'value_num'].sum(skipna=True)

def edca_ac_breakdown(all_sca_csv_path: str):
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = df['value'].apply(to_float_safe)

    m = df['module'].str.contains(r'mac\.hcf\.edca\.edcaf\[\d+\]', case=False, na=False)
    n = df['name'].isin(['packetSentToPeerWithRetry:count', 'packetSentToPeerWithoutRetry:count'])
    g = df.loc[m & n, ['module', 'name', 'value_num']].copy()
    if g.empty:
        return g

    # edcaf ì¸ë±ìŠ¤ ì¶”ì¶œ â†’ AC ë¼ë²¨ë§
    g['ac'] = pd.to_numeric(g['module'].str.extract(r'edcaf\[(\d+)\]', expand=False), errors='coerce').astype('Int64')
    g['ac_name'] = g['ac'].map(AC_MAP)

    g = (g.groupby(['ac', 'ac_name', 'module', 'name'], dropna=False)['value_num']
           .sum()
           .reset_index()
           .sort_values(['ac', 'name', 'module']))

    if not g.empty:
        print("\n[EDCA per-AC ì „ì†¡ í”„ë ˆì„ í•©ê³„]")
        print(g.to_string(index=False))
    return g


def compute_avg_bps_fallback_from_scalars(all_sca_csv_path: str,
                                          sim_time_limit_sec: float = 30.0):
    """
    CSV-R ë²¡í„°(throughput)ê°€ ë¹„ì–´ ìˆì„ ë•Œ UDP Sinkì˜
    rcvdPk:sum(packetBytes) ìŠ¤ì¹¼ë¼ í•©ê³„ë¡œ í‰ê·  bps ê³„ì‚°.
    """
    df = read_scavetool_csv(all_sca_csv_path)
    m = df['name'].astype(str).str.contains(r'rcvdPk:sum\(packetBytes\)', case=False, na=False)
    total_bytes = pd.to_numeric(df.loc[m, 'value'], errors='coerce').fillna(0).sum()
    avg_bps = 8.0 * total_bytes / sim_time_limit_sec if sim_time_limit_sec > 0 else 0.0
    print(f"ğŸ“ˆ[fallback] UDP ìˆ˜ì‹  í•©ê³„={total_bytes:.0f} bytes â†’ í‰ê·  {avg_bps:.2f} bps")
    return avg_bps


def total_retrycount_scalar(all_sca_csv_path: str) -> float:
    """
    ì¼ë¶€ ëŸ°ì—ì„œ ì œê³µë˜ëŠ” retryCount ìŠ¤ì¹¼ë¼ í•©ê³„ ì§‘ê³„.
    (ìˆìœ¼ë©´ ì¬ì‹œë„ ê²½í–¥ íŒŒì•…ì— ë„ì›€ì´ ë¨)
    """
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = pd.to_numeric(df['value'], errors='coerce')
    return df.loc[df['name'] == 'retryCount', 'value_num'].sum(skipna=True)


def main():
    print("--- OMNeT++ ê²°ê³¼ ë°ì´í„° í›„ì²˜ë¦¬ ì‹œì‘ ---")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {OUTPUT_DIR}\n")

    vec_files, sca_files = ensure_files()

    # 1) ì „ì²´ ë¤í”„
    all_vec_csv = os.path.join(OUTPUT_DIR, 'all_vectors.csv')
    all_sca_csv = os.path.join(OUTPUT_DIR, 'all_scalars.csv')
    dump_all(vec_files, sca_files, all_vec_csv, all_sca_csv)

    # 2) Throughput ë²¡í„° ì¶”ì¶œ
    thr_csv = os.path.join(OUTPUT_DIR, 'throughput.csv')
    filter_vectors(all_vec_csv, thr_csv,
                   name_keys=THR_NAME_KEYS,
                   module_keys=THR_MODULE_KEYS)

    # 3) CW ë²¡í„° ì¶”ì¶œ
    cw_csv = os.path.join(OUTPUT_DIR, 'cw_data.csv')
    filter_vectors(all_vec_csv, cw_csv,
                   name_keys=CW_NAME_KEYS)

    # 4) Retry ìŠ¤ì¹¼ë¼ ì¶”ì¶œ (ê·¸ëŒ€ë¡œ ì €ì¥)
    retry_csv = os.path.join(OUTPUT_DIR, 'retry_counts.csv')
    filter_rows(all_sca_csv, retry_csv,
                name_exact=RETRY_NAMES)

    # 5) Packet Loss Rate ê³„ì‚° (MAC ì¬ì‹œë„ í•œê³„ ê¸°ì¤€)
    total_sent    = total_tx_frames(all_sca_csv)                # âœ… ìˆ˜ì •
    total_dropped = total_retrylimit_drops(all_sca_csv)         # âœ… ìˆ˜ì •
    arp_failed    = total_arp_fail(all_sca_csv)                 # âœ… ì¶”ê°€

    loss_rate = (total_dropped / total_sent * 100.0) if total_sent > 0 else 0.0

    # 6) ìŠ¤ë£¨í’‹/ì¬ì‹œë„/CW ìš”ì•½
    out = compute_avg_bps(thr_csv, os.path.join(OUTPUT_DIR, 'throughput_bps.csv'))
    if out is None or getattr(out, 'empty', True):
        # sim-time-limit(ê¸°ë³¸ 30s)ì— ë§ì¶° ì¡°ì • ê°€ëŠ¥
        compute_avg_bps_fallback_from_scalars(all_sca_csv, sim_time_limit_sec=5.0)

    compute_retry_ratio(all_sca_csv)
    cw_change_summary(cw_csv)
    retry_count_sum = total_retrycount_scalar(all_sca_csv)

    summary = (
        "----- ìš”ì•½ -----\n"
        f"- ì´ ì „ì†¡ í”„ë ˆì„(EDCA/DCF í•©): {total_sent:.0f}\n"
        f"- MAC ì¬ì‹œë„ í•œê³„ ë“œë: {total_dropped:.0f}\n"
        f"- íŒ¨í‚· ì†ì‹¤ë¥ (=ì¬ì‹œë„í•œê³„ë“œë/ì „ì†¡): {loss_rate:.2f}%\n"
        f"- IP ì£¼ì†Œí•´ê²° ì‹¤íŒ¨(ARP ì‹¤íŒ¨): {arp_failed:.0f}\n"
        f"- retryCount(ìŠ¤ì¹¼ë¼ í•©): {retry_count_sum:.0f}\n"
    )
    print(summary)

    with open(os.path.join(OUTPUT_DIR, 'packet_loss_summary.txt'), 'w', encoding='utf-8') as f:
        f.write(summary.strip())

    # í•„ìš” ì‹œ ë“œë¡­ ìŠ¤ìº” í™œì„±í™”
    # scan_drops(all_sca_csv)

    # edca_ac_breakdown(all_sca_csv)

   # ---------- MAC íš¨ìœ¨ ê³„ì‚° ----------  â† ê¸°ì¡´ ë¸”ë¡ì„ ì „ë¶€ ëŒ€ì²´ -----------------
    # (ì˜ˆ) IEEE 802.11g 12 Mbps Baselineìš© ìƒìˆ˜
    PHY_DATA_RATE      = 12_000_000  # bps
    PHY_SLOT_TIME      = 9e-6        # 9 Âµs
    PHY_RIFS           = 2e-6        # 802.11g RIFS 2 Âµs (í•„ìš” ì—†ìœ¼ë©´ 0)
    PHY_CIFS           = 34e-6       # DIFS â‰ˆ 34 Âµs
    PHY_ACK_TIME       = 44e-6       # PHY preamble+ACK bits @24 Mbps
    PKT_BYTES          = 1500

    # ì‹ í˜¸ ì „ì†¡ì‹œê°„ = í”„ë¦¬ì•°ë¸”/PHY í—¤ë” + ë°ì´í„°/ACK ì „ì†¡ì‹œê°„
    def tx_time(payload_bytes, rate_bps):
        return (payload_bytes * 8) / rate_bps      # data-only (PHY í—¤ë” ë“±ì€ ë¬´ì‹œ)

    T_FRAME = tx_time(PKT_BYTES, PHY_DATA_RATE)    # ë°ì´í„° í”„ë ˆì„
    T_ACK   = PHY_ACK_TIME
    T_S     = T_FRAME + PHY_RIFS + T_ACK + PHY_CIFS      # ì„±ê³µ ê¸°ê°„
    T_C     = T_FRAME + PHY_CIFS                         # ì¶©ëŒ ê¸°ê°„
    T_I     = PHY_SLOT_TIME                              # idle ê¸°ê°„

    # === 1) ì„±ê³µÂ·ì¶©ëŒÂ·ì „ì†¡ ì¹´ìš´íŠ¸ ===
    succ_pkts   = sum_scalar(all_sca_csv, 'packetReceivedFromPeer:count')
    tx_with_r   = sum_scalar(all_sca_csv, 'packetSentToPeerWithRetry:count')
    tx_wo_r     = sum_scalar(all_sca_csv, 'packetSentToPeerWithoutRetry:count')
    tx_attempts = tx_with_r + tx_wo_r
    collisions  = total_retrylimit_drops(all_sca_csv)     # = ì¬ì‹œë„ í•œê³„ ì´ˆê³¼
    idle_slots  = 0                                       # (ë³„ë„ ë²¡í„°ë¥¼ ëŒì–´ì˜¤ë©´ ì±„ì›Œì£¼ì„¸ìš”)

    if tx_attempts == 0:
        print("âš ï¸  ì „ì†¡ ì‹œë„ê°€ 0 ì…ë‹ˆë‹¤. Î· ê³„ì‚° ë¶ˆê°€")
        eta = 0.0
    else:
        P_S = succ_pkts / tx_attempts
        P_C = collisions / tx_attempts
        P_I = 1.0 - P_S - P_C                                # ë‚¨ëŠ” ì‹œê°„ì„ idle ë¡œ ê°€ì •

        # === 2) ì‹ (4) ì ìš© ===
        E_DATA_bits = PKT_BYTES * 8
        numerator   = P_S * E_DATA_bits
        denominator = (P_S * T_S + P_I * T_I + P_C * T_C) * PHY_DATA_RATE
        eta         = numerator / denominator if denominator else 0.0

        print(f"P_S={P_S:.4f}, P_I={P_I:.4f}, P_C={P_C:.4f}")
        print(f"T_S={T_S*1e6:.2f} Âµs  T_I={T_I*1e6:.2f} Âµs  T_C={T_C*1e6:.2f} Âµs")

    print(f"ğŸ¯  MAC íš¨ìœ¨ Î· â‰ˆ {eta:.3f}")
    print("âœ… ì™„ë£Œ")
    # --------------------------------------------------------------------------



if __name__ == "__main__":
    main()
