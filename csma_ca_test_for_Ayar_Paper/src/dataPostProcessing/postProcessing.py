#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OMNeT++ 6.1 / opp_scavetool ê²°ê³¼ í›„ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- í•„í„° ì—†ì´ ì „ì²´ ë¤í”„(CSV-R, CSV-S) â†’ Pythonì—ì„œ name/moduleë¡œ í•„í„°ë§
"""

import os
import sys
import glob
import csv
import io
import subprocess
import pandas as pd
from typing import List, Tuple
# íŒŒì¼ ìƒë‹¨ importë“¤ ì•„ë˜ì— ì¶”ê°€
import re
from pathlib import Path


# EDCA AC ì¸ë±ìŠ¤ â†’ ì´ë¦„ ë§¤í•‘
AC_MAP = {0: 'AC_VO', 1: 'AC_VI', 2: 'AC_BE', 3: 'AC_BK'}

# ------------------------- ì‚¬ìš©ì ì„¤ì • -------------------------
SCRIPT_DIR   = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR   = os.path.join(SCRIPT_DIR, 'data')
RESULTS_DIR  = os.path.join(SCRIPT_DIR, '..', '..', 'ini', 'results')

# ë²¡í„° í•„í„° í‚¤ì›Œë“œ(ë¶€ë¶„ ë¬¸ìì—´ AND)
THR_NAME_KEYS    = ['packetReceivedFromPeer']   # with/withoutRetry ëª¨ë‘ ë§¤ì¹˜
THR_MODULE_KEYS  = []                           # ëª¨ë“ˆ ì œí•œ ì—†ìŒ
CW_NAME_KEYS     = ['contentionWindowChanged']  # EDCA/DCF ê³µí†µ í‚¤ì›Œë“œ

# ìŠ¤ì¹¼ë¼ ì´ë¦„(ì •í™• ì¼ì¹˜)
RETRY_NAMES = [
    'packetSentToPeerWithRetry:count',
    'packetSentToPeerWithoutRetry:count',
    # í•„ìš” ì‹œ ë°”ì´íŠ¸ í•©ê³„ë„ í™•ì¸ ê°€ëŠ¥
    # 'packetSentToPeerWithRetry:sum(packetBytes)',
    # 'packetSentToPeerWithoutRetry:sum(packetBytes)',
]

# âœ… DCF ì „ìš© ì§€í‘œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# SENT_NAME / SENT_MODULE_KEY ì œê±°
# DROP_NAME / DROP_MODULE_KEYë„ ëª¨ë“ˆ ì œí•œì„ ë‘ì§€ ì•ŠìŠµë‹ˆë‹¤.
DROP_NAME = 'retryLimitReached'   # INET 4.5.4 í˜¸í™˜
# ---------------------------------------------------------------


# ===== MAC íš¨ìœ¨(ë…¼ë¬¸ ì‹ (4)) ê³„ì‚° í—¬í¼ =====

def _tx_time_bytes(payload_bytes, rate_bps):
    return (payload_bytes * 8.0) / float(rate_bps)

def compute_mac_efficiency_from_results(
    all_sca_csv: str,
    thr_df,             # compute_avg_bps() ê²°ê³¼ DataFrame (sum_bytes/avg_bps í¬í•¨)
    phy_rate_bps=12_000_000,   # Baseline: 12 Mbps
    slot_time=9e-6,            # 802.11g slot
    rifs=2e-6,                 # 802.11g RIFS
    cifs=34e-6,                # DIFS(ìœ ì‚¬ì¹˜)
    ack_time=44e-6,            # preamble+ACK ì‹œê°„(ê·¼ì‚¬)
    pkt_bytes=1500,            # ë°ì´í„° í˜ì´ë¡œë“œ ë°”ì´íŠ¸
    verbose=True
    ):
    """
    ë…¼ë¬¸ ì‹(4) Î· = P_S*E[Data] / (P_S*T_S + P_I*T_I + P_C*T_C) / R ë¡œ ê³„ì‚°.
    P_S: ì„±ê³µí™•ë¥ , P_C: ì¶©ëŒ/ë“œë¡­ í™•ë¥ , P_I: idle í™•ë¥ (1-P_S-P_C)
    ì¹´ìš´íŠ¸ë¥˜ëŠ” ìŠ¤ì¹¼ë¼ì—ì„œ ê°€ì ¸ì˜¤ê³ , ì—†ì„ ê²½ìš° thr_dfë¡œ ì¶”ì •.
    """
    # (1) ì¹´ìš´íŠ¸ ì¶”ì¶œ
    succ_pkts_scalar = sum_scalar(all_sca_csv, 'packetReceivedFromPeer:count')  # ì—†ìœ¼ë©´ None
    tx_with_r   = sum_scalar(all_sca_csv, 'packetSentToPeerWithRetry:count') or 0
    tx_wo_r     = sum_scalar(all_sca_csv, 'packetSentToPeerWithoutRetry:count') or 0
    tx_attempts = tx_with_r + tx_wo_r

    # ë“œë¡­(ì¬ì‹œë„ í•œê³„) ì¹´ìš´íŠ¸: ë¹Œë“œì— ë”°ë¼ ì—†ì„ ìˆ˜ ìˆìŒ â†’ ì—†ìœ¼ë©´ 0
    retry_limit = sum_scalar(all_sca_csv, 'retryLimitReached:count') or 0

    # (2) succ_pkts ë³´ì •: ìŠ¤ì¹¼ë¼ê°€ ì—†ìœ¼ë©´ thr_dfì—ì„œ ì¶”ì •
    succ_bytes = 0
    if thr_df is not None and not thr_df.empty:
        if 'sum_bytes' in thr_df.columns:
            succ_bytes = float(thr_df['sum_bytes'].sum())
    if succ_pkts_scalar is not None:
        succ_pkts = float(succ_pkts_scalar)
    else:
        succ_pkts = (succ_bytes / float(pkt_bytes)) if pkt_bytes > 0 else 0.0

    # ë³´í˜¸: tx_attemptsê°€ 0ì´ë©´ íš¨ìœ¨ ê³„ì‚° ë¶ˆê°€
    if tx_attempts <= 0:
        if verbose:
            print("âš ï¸  ì „ì†¡ ì‹œë„ê°€ 0ì…ë‹ˆë‹¤. Î· ê³„ì‚°ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return {
            'eta': 0.0, 'P_S': 0.0, 'P_I': 0.0, 'P_C': 0.0,
            'T_S': 0.0, 'T_I': slot_time, 'T_C': 0.0,
            'succ_pkts': 0, 'tx_attempts': 0, 'collisions': 0
        }

    # (3) í™•ë¥  ê³„ì‚°
    P_S = succ_pkts / tx_attempts if tx_attempts > 0 else 0.0
    P_C = (retry_limit / tx_attempts) if tx_attempts > 0 else 0.0  # ì¬ì‹œë„í•œê³„ë„ë‹¬ì„ ì¶©ëŒ/ì‹¤íŒ¨ë¡œ ì§‘ê³„
    P_I = max(0.0, 1.0 - P_S - P_C)

    # (4) ì‹œê°„ ìƒìˆ˜
    T_FRAME = _tx_time_bytes(pkt_bytes, phy_rate_bps)
    T_ACK   = ack_time
    T_S     = T_FRAME + rifs + T_ACK + cifs
    T_C     = T_FRAME + cifs
    T_I     = slot_time

    # (5) ì‹ (4)
    E_DATA_bits = pkt_bytes * 8.0
    numerator   = P_S * E_DATA_bits
    denominator = (P_S * T_S + P_I * T_I + P_C * T_C) * phy_rate_bps
    eta         = (numerator / denominator) if denominator > 0 else 0.0

    if verbose:
        print(f"P_S={P_S:.4f}, P_I={P_I:.4f}, P_C={P_C:.4f}")
        print(f"T_S={T_S*1e6:.2f} Âµs  T_I={T_I*1e6:.2f} Âµs  T_C={T_C*1e6:.2f} Âµs")
        print(f"ğŸ¯  MAC íš¨ìœ¨ Î· â‰ˆ {eta:.3f}")

    return {
        'eta': eta, 'P_S': P_S, 'P_I': P_I, 'P_C': P_C,
        'T_S': T_S, 'T_I': T_I, 'T_C': T_C,
        'succ_pkts': int(succ_pkts), 'tx_attempts': int(tx_attempts), 'collisions': int(retry_limit)
    }


def write_packet_loss_summary_txt(path: str, n: int, metrics: dict, extra: dict):
    """
    packet_loss_summary.txt ì‘ì„±
    """
    lines = []
    lines.append(f"n={n}")
    lines.append(f"ì´ ì „ì†¡ ì‹œë„: {metrics.get('tx_attempts',0)}")
    lines.append(f"ì„±ê³µ íŒ¨í‚·: {metrics.get('succ_pkts',0)}")
    lines.append(f"ì¶©ëŒ/ë“œë¡­(ì¬ì‹œë„í•œê³„): {metrics.get('collisions',0)}")
    lines.append(f"P_S={metrics.get('P_S',0):.4f}, P_I={metrics.get('P_I',0):.4f}, P_C={metrics.get('P_C',0):.4f}")
    lines.append(f"T_S={metrics.get('T_S',0)*1e6:.2f} Âµs  T_I={metrics.get('T_I',0)*1e6:.2f} Âµs  T_C={metrics.get('T_C',0)*1e6:.2f} Âµs")
    lines.append(f"MAC íš¨ìœ¨ Î· â‰ˆ {metrics.get('eta',0):.3f}")
    # ë¶€ê°€ ì •ë³´
    if 'avg_bps' in extra:
        lines.append(f"í‰ê·  ìŠ¤ë£¨í’‹(bps-ë²¡í„°í‰ê· ): {extra['avg_bps']:.2f}")
    if 'retry_with' in extra and 'retry_without' in extra:
        tot = (extra['retry_with'] or 0) + (extra['retry_without'] or 0)
        rw  = extra['retry_with'] or 0
        pct = (rw / tot * 100.0) if tot > 0 else 0.0
        lines.append(f"ì¬ì‹œë„ í”„ë ˆì„: {rw}/{tot}  ({pct:.2f}%)")

    with open(path, 'w', encoding='utf-8') as f:
        f.write("\n".join(lines) + "\n")


def run_cmd(cmd: List[str], desc: str) -> str:
    print(f"ğŸš€ {desc}...")
    print("   $", " ".join(cmd))
    try:
        p = subprocess.run(cmd, check=True, text=True,
                           capture_output=True, encoding='utf-8')
        if p.stdout.strip():
            print(p.stdout.strip())
        print("âœ… ì™„ë£Œ\n")
        return p.stdout
    except FileNotFoundError:
        print(f"âŒ '{cmd[0]}' ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PATH í™•ì¸.")
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"âŒ ì—ëŸ¬: {desc}")
        print("---- stdout ----")
        print(e.stdout)
        print("---- stderr ----")
        print(e.stderr)
        sys.exit(1)


def ensure_files() -> Tuple[List[str], List[str]]:
    vec_files = glob.glob(os.path.join(RESULTS_DIR, '*.vec'))
    sca_files = glob.glob(os.path.join(RESULTS_DIR, '*.sca'))
    if not vec_files and not sca_files:
        print(f"âŒ .vec/.sca ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {RESULTS_DIR}")
        sys.exit(1)
    print(f"ğŸ” vec files: {len(vec_files)}ê°œ / sca files: {len(sca_files)}ê°œ ë°œê²¬")
    return vec_files, sca_files


def dump_all(vec_files: List[str], sca_files: List[str],
             vec_out: str, sca_out: str):
    # ë²¡í„° ì „ì²´
    cmd_v = ['opp_scavetool', 'x', '-v', '-F', 'CSV-R', '-o', vec_out] + vec_files
    run_cmd(cmd_v, "ë²¡í„° ì „ì²´ ë¤í”„")

    # ìŠ¤ì¹¼ë¼+íŒŒë¼ë¯¸í„° ì „ì²´
    cmd_s = ['opp_scavetool', 'x', '-F', 'CSV-S', '-x', 'allowMixed=true', '-o', sca_out] + sca_files
    run_cmd(cmd_s, "ìŠ¤ì¹¼ë¼/íŒŒë¼ë¯¸í„° ì „ì²´ ë¤í”„")


def read_scavetool_csv(path: str) -> pd.DataFrame:
    """opp_scavetool CSVì—ì„œ í—¤ë” ì¤„ì„ ì°¾ì•„ pandasë¡œ ì½ëŠ”ë‹¤"""
    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()
    header_idx = None
    for i, l in enumerate(lines):
        if l.strip().startswith('run,') and ',module,' in l and ',name,' in l:
            header_idx = i
            break
    if header_idx is None:
        raise ValueError(f"Header not found in {path}")
    csv_text = ''.join(lines[header_idx:])
    return pd.read_csv(io.StringIO(csv_text), engine='python', on_bad_lines='skip')

def filter_vectors(in_csv: str, out_csv: str,
                   name_keys: List[str] = None,
                   module_keys: List[str] = None):
    name_keys   = name_keys or []
    module_keys = module_keys or []

    df = read_scavetool_csv(in_csv)

    # í—¤ë” ì»¨í…ìŠ¤íŠ¸ ì „íŒŒ
    for col in ['run', 'module', 'name']:
        if col in df.columns:
            df[col] = df[col].replace('', pd.NA).ffill()

    # CSV-R ë²¡í„° ì»¬ëŸ¼ í™•ì¸
    if 'vectime' in df.columns and 'vecvalue' in df.columns:
        tcol, vcol = 'vectime', 'vecvalue'
        # âœ… ìˆ«ì ë³€í™˜ ê¸ˆì§€! ë¬¸ìì—´ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ë§Œ í™•ì¸
        sel = df[tcol].astype(str).str.strip().ne('') & df[vcol].astype(str).str.strip().ne('')
    else:
        # (CSV â€˜ìƒ˜í”Œë‹¹ 1í–‰â€™ í˜•ì‹ì¼ ë•Œë§Œ ìˆ«ì í™•ì¸)
        tcol, vcol = 'time', 'value'
        sel = pd.to_numeric(df[tcol], errors='coerce').notna() & pd.to_numeric(df[vcol], errors='coerce').notna()

    # ì´ë¦„/ëª¨ë“ˆ í‚¤ì›Œë“œ(AND)
    for k in name_keys:
        sel &= df['name'].str.contains(k, case=False, na=False)
    for k in module_keys:
        sel &= df['module'].str.contains(k, case=False, na=False)

    kept = df.loc[sel].copy()
    kept.to_csv(out_csv, index=False)
    print(f"â„¹ï¸  {os.path.basename(out_csv)}: {len(kept)} vector data rows kept "
          f"(name_keys={name_keys}, module_keys={module_keys})")


def filter_rows(in_csv: str, out_csv: str,
                name_keys: List[str] = None,
                module_keys: List[str] = None,
                name_exact: List[str] = None):
    """ë¶€ë¶„/ì •í™• ì¼ì¹˜ë¡œ í–‰ í•„í„°ë§"""
    name_keys   = name_keys or []
    module_keys = module_keys or []
    name_exact  = name_exact or []

    df = read_scavetool_csv(in_csv)

    sel = pd.Series([True] * len(df))
    if name_exact:
        sel &= df['name'].isin(name_exact)

    if name_keys:
        for k in name_keys:
            sel &= df['name'].str.contains(k, case=False, na=False)

    if module_keys:
        for k in module_keys:
            sel &= df['module'].str.contains(k, case=False, na=False)

    kept_df = df[sel].copy()
    kept_df.to_csv(out_csv, index=False)
    print(f"â„¹ï¸  {os.path.basename(out_csv)}: {len(kept_df)} rows kept "
          f"(name_exact={name_exact}, name_keys={name_keys}, module_keys={module_keys})")


def to_float_safe(x):
    try:
        return float(x)
    except Exception:
        return None


def sum_scalar(in_csv: str, target_name: str, module_key: str = None) -> float:
    df = read_scavetool_csv(in_csv)
    df['value_num'] = df['value'].apply(to_float_safe)
    sel = df['name'] == target_name
    if module_key:
        sel &= df['module'].str.contains(module_key, case=False, na=False)
    return df.loc[sel, 'value_num'].sum(skipna=True)

def _parse_series(cell: str):
    if not isinstance(cell, str):
        return []
    s = cell.strip().replace('[',' ').replace(']',' ').replace(',',' ')
    parts = [p for p in s.split() if p]
    try:
        return [float(p) for p in parts]
    except ValueError:
        return []

def compute_avg_bps(throughput_csv_path, out_csv_path):
    df = read_scavetool_csv(throughput_csv_path)
    if df.empty:
        print("ğŸ“ˆ ìŠ¤ë£¨í’‹ ê³„ì‚°: ì…ë ¥ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    rows = []
    if 'vectime' in df.columns and 'vecvalue' in df.columns:
        # âœ… CSV-R: í•œ í–‰ì— ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŒ
        for _, r in df.iterrows():
            times = _parse_series(r.get('vectime'))
            vals  = _parse_series(r.get('vecvalue'))
            if len(times) >= 2 and len(vals) == len(times):
                duration = times[-1] - times[0]
                totalB   = sum(vals)
                if duration > 0:
                    rows.append({
                        'run': r['run'], 'module': r['module'], 'name': r['name'],
                        'samples': len(times), 'duration': duration,
                        'sum_bytes': totalB, 'avg_bps': 8.0 * totalB / duration
                    })
    else:
        # (CSV â€˜ìƒ˜í”Œë‹¹ 1í–‰â€™ í˜•ì‹ì¼ ë•Œ)
        tcol = 'time'; vcol = 'value'
        df[tcol] = pd.to_numeric(df[tcol], errors='coerce')
        df[vcol] = pd.to_numeric(df[vcol], errors='coerce')
        df = df.dropna(subset=[tcol, vcol])
        for (r, m, n), g in df.groupby(['run','module','name']):
            g = g.sort_values(tcol)
            if len(g) < 2:
                continue
            duration = g[tcol].iloc[-1] - g[tcol].iloc[0]
            totalB   = g[vcol].sum()
            if duration > 0:
                rows.append({'run': r, 'module': m, 'name': n,
                             'samples': len(g), 'duration': duration,
                             'sum_bytes': totalB, 'avg_bps': 8.0 * totalB / duration})

    out = pd.DataFrame(rows)
    out.to_csv(out_csv_path, index=False)
    if not out.empty:
        print(f"ğŸ“ˆ í‰ê·  ìŠ¤ë£¨í’‹ rows: {len(out)}  (ì €ì¥: {out_csv_path})")
        print(f"   ì „ì²´ í‰ê· : {out['avg_bps'].mean():.2f} bps")
    else:
        print("ğŸ“ˆ ìŠ¤ë£¨í’‹ ê³„ì‚°í•  í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
    return out


def compute_retry_ratio(all_sca_csv_path):
    """ì¬ì‹œë„ í”„ë ˆì„ ë¹„ìœ¨(%) ê³„ì‚°"""
    df = read_scavetool_csv(all_sca_csv_path)
    w  = df.loc[df['name']=='packetSentToPeerWithRetry:count','value'].astype(float).sum()
    wo = df.loc[df['name']=='packetSentToPeerWithoutRetry:count','value'].astype(float).sum()
    tot = w + wo
    ratio = (w / tot * 100.0) if tot > 0 else 0.0
    print(f"ğŸ” ì¬ì‹œë„ í”„ë ˆì„: {w:.0f}/{tot:.0f}  ({ratio:.2f}%)")
    return ratio

def cw_change_summary(cw_csv_path):
    """CW ë³€ê²½ ì´ë²¤íŠ¸ ê°œìˆ˜(ë¹ ë¥¸ sanity ì²´í¬)"""
    df = read_scavetool_csv(cw_csv_path)
    print(f"âš™ï¸ CW ë³€ê²½ ì´ë²¤íŠ¸ ìˆ˜: {len(df)}")
    return len(df)

def scan_drops(all_sca_csv_path):
    df = read_scavetool_csv(all_sca_csv_path)
    cand = df[df['name'].astype(str).str.contains('drop|queue|overflow|discard|fail', case=False, na=False)].copy()
    cand['value_num'] = pd.to_numeric(cand['value'], errors='coerce')

    # í•©ê³„ TOP10 ë¨¼ì €
    top = (cand.groupby('name')['value_num']
               .sum()
               .sort_values(ascending=False)
               .head(10))
    print("\nğŸ” Drop-like ì§€í‘œ TOP10(í•©ê³„):")
    print(top.to_string())

    # ì˜ˆì‹œ 20í–‰ ë¯¸ë¦¬ë³´ê¸°
    print("\nğŸ” ì˜ˆì‹œ 20í–‰ ë¯¸ë¦¬ë³´ê¸°:")
    print(cand[['module','name','value']].head(20).to_string(index=False))
    return cand


# ---- postProcessing.py (ì¼ë¶€) ----
def total_tx_frames(all_sca_csv_path: str) -> float:
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = df['value'].apply(to_float_safe)
    # EDCAÂ·DCF ê³µí†µìœ¼ë¡œ ì´ë¦„ë§Œ ë³´ê³  í•©ê³„
    return df[df['name'].isin(RETRY_NAMES)]['value_num'].sum(skipna=True)



def total_retrylimit_drops(all_sca_csv_path: str) -> float:
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = df['value'].apply(to_float_safe)
    mask = (df['name'] == 'packetDropRetryLimitReached:count')
    return df.loc[mask, 'value_num'].sum(skipna=True)

def total_arp_fail(all_sca_csv_path: str) -> float:
    """IP ê³„ì¸µì˜ ì£¼ì†Œí•´ê²° ì‹¤íŒ¨(ARP ì‹¤íŒ¨) ì´í•©."""
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = df['value'].apply(to_float_safe)
    mask = (df['name'] == 'packetDropAddressResolutionFailed:count')
    return df.loc[mask, 'value_num'].sum(skipna=True)

def edca_ac_breakdown(all_sca_csv_path: str):
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = df['value'].apply(to_float_safe)

    m = df['module'].str.contains(r'mac\.hcf\.edca\.edcaf\[\d+\]', case=False, na=False)
    n = df['name'].isin(['packetSentToPeerWithRetry:count', 'packetSentToPeerWithoutRetry:count'])
    g = df.loc[m & n, ['module', 'name', 'value_num']].copy()
    if g.empty:
        return g

    # edcaf ì¸ë±ìŠ¤ ì¶”ì¶œ â†’ AC ë¼ë²¨ë§
    g['ac'] = pd.to_numeric(g['module'].str.extract(r'edcaf\[(\d+)\]', expand=False), errors='coerce').astype('Int64')
    g['ac_name'] = g['ac'].map(AC_MAP)

    g = (g.groupby(['ac', 'ac_name', 'module', 'name'], dropna=False)['value_num']
           .sum()
           .reset_index()
           .sort_values(['ac', 'name', 'module']))

    if not g.empty:
        print("\n[EDCA per-AC ì „ì†¡ í”„ë ˆì„ í•©ê³„]")
        print(g.to_string(index=False))
    return g


def compute_avg_bps_fallback_from_scalars(all_sca_csv_path: str,
                                          sim_time_limit_sec: float = 30.0):
    """
    CSV-R ë²¡í„°(throughput)ê°€ ë¹„ì–´ ìˆì„ ë•Œ UDP Sinkì˜
    rcvdPk:sum(packetBytes) ìŠ¤ì¹¼ë¼ í•©ê³„ë¡œ í‰ê·  bps ê³„ì‚°.
    """
    df = read_scavetool_csv(all_sca_csv_path)
    m = df['name'].astype(str).str.contains(r'rcvdPk:sum\(packetBytes\)', case=False, na=False)
    total_bytes = pd.to_numeric(df.loc[m, 'value'], errors='coerce').fillna(0).sum()
    avg_bps = 8.0 * total_bytes / sim_time_limit_sec if sim_time_limit_sec > 0 else 0.0
    print(f"ğŸ“ˆ[fallback] UDP ìˆ˜ì‹  í•©ê³„={total_bytes:.0f} bytes â†’ í‰ê·  {avg_bps:.2f} bps")
    return avg_bps


def total_retrycount_scalar(all_sca_csv_path: str) -> float:
    """
    ì¼ë¶€ ëŸ°ì—ì„œ ì œê³µë˜ëŠ” retryCount ìŠ¤ì¹¼ë¼ í•©ê³„ ì§‘ê³„.
    (ìˆìœ¼ë©´ ì¬ì‹œë„ ê²½í–¥ íŒŒì•…ì— ë„ì›€ì´ ë¨)
    """
    df = read_scavetool_csv(all_sca_csv_path)
    df['value_num'] = pd.to_numeric(df['value'], errors='coerce')
    return df.loc[df['name'] == 'retryCount', 'value_num'].sum(skipna=True)

def collect_runs_by_n(results_dir: str, conf_prefix: str = "Paper_Baseline"):
    """
    results_dir ì•ˆì˜ .vec/.sca íŒŒì¼ì„ ìŠ¤ìº”í•´ì„œ
    íŒŒì¼ëª… íŒ¨í„´:  <conf_prefix>-n=<N>-#<run>.{vec|sca}
    ë¥¼ ì°¾ì•„ N(ë…¸ë“œ ìˆ˜)ë³„ë¡œ ë¬¶ì–´ return.

    return ì˜ˆì‹œ:
    {
      5:  {'vec': ['/.../Paper_Baseline-n=5-#0.vec'], 'sca': ['/.../Paper_Baseline-n=5-#0.sca']},
      20: {'vec': ['/.../Paper_Baseline-n=20-#0.vec'], 'sca': ['/.../Paper_Baseline-n=20-#0.sca']},
      ...
    }
    """
    runs = {}
    p = Path(results_dir)
    for f in p.glob(f"{conf_prefix}-n=*#*.*"):
        m = re.search(rf"{re.escape(conf_prefix)}-n=(\d+)-#\d+\.(vec|sca)$", f.name)
        if not m:
            continue
        n = int(m.group(1))
        ext = m.group(2)
        runs.setdefault(n, {'vec': [], 'sca': []})
        runs[n][ext].append(str(f))
    # ê° në§ˆë‹¤ vec/sca ìµœì‹  ê²ƒ 1ê°œë§Œ ì“°ë„ë¡ ì •ë¦¬(ì—¬ëŸ¬ run ìˆì„ ë•Œ)
    for n in list(runs.keys()):
        runs[n]['vec'] = sorted(runs[n]['vec'])[-1:] if runs[n]['vec'] else []
        runs[n]['sca'] = sorted(runs[n]['sca'])[-1:] if runs[n]['sca'] else []
        if not runs[n]['vec'] and not runs[n]['sca']:
            runs.pop(n, None)
    return runs

def make_outputs_for_n(output_dir: str, n: int):
    """
    n ê°’ë³„ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ë¬¶ìŒ ë°˜í™˜
    """
    odir = Path(output_dir) / f"n{n}"
    odir.mkdir(parents=True, exist_ok=True)
    return {
        'out_dir':   str(odir),                     # <- ì¶”ê°€
        'all_vec_csv': str(odir / "all_vectors.csv"),
        'all_sca_csv': str(odir / "all_scalars.csv"),
        'thr_csv':     str(odir / "throughput.csv"),
        'cw_csv':      str(odir / "cw_data.csv"),
        'retry_csv':   str(odir / "retry_counts.csv"),
        'thr_bps_csv': str(odir / "throughput_bps.csv"),
    }

def main():
    eta_rows = []   # në³„ MAC íš¨ìœ¨ ë¹„êµìš© ëˆ„ì 

    print("--- OMNeT++ ê²°ê³¼ ë°ì´í„° í›„ì²˜ë¦¬ ì‹œì‘ ---")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {OUTPUT_DIR}\n")

    # 1) n ê°’ë³„ run ìˆ˜ì§‘ (ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ fallback)
    runs = collect_runs_by_n(RESULTS_DIR, conf_prefix="Paper_Baseline")

    if not runs:
        # ê¸°ì¡´ ë™ì‘: í•œ ì„¸íŠ¸ë§Œ ì²˜ë¦¬
        vec_files, sca_files = ensure_files()
        all_vec_csv = os.path.join(OUTPUT_DIR, 'all_vectors.csv')
        all_sca_csv = os.path.join(OUTPUT_DIR, 'all_scalars.csv')
        dump_all(vec_files, sca_files, all_vec_csv, all_sca_csv)

        # í•„í„°ë§
        thr_csv = os.path.join(OUTPUT_DIR, 'throughput.csv')
        cw_csv = os.path.join(OUTPUT_DIR, 'cw_data.csv')
        retry_csv = os.path.join(OUTPUT_DIR, 'retry_counts.csv')
        filter_vectors(all_vec_csv, thr_csv, THR_NAME_KEYS, THR_MODULE_KEYS)
        filter_vectors(all_vec_csv, cw_csv, CW_NAME_KEYS, [])
        filter_rows(all_sca_csv, retry_csv, name_exact=RETRY_NAMES)

        # ìŠ¤ë£¨í’‹ ê³„ì‚°
        thr_bps_csv = os.path.join(OUTPUT_DIR, 'throughput_bps.csv')
        compute_avg_bps(thr_csv, thr_bps_csv)

        # ìš”ì•½(ê¸°ì¡´ ì¶œë ¥ ë£¨í‹´ ìœ ì§€)
        retry_with = sum_scalar(all_sca_csv, 'packetSentToPeerWithRetry:count')
        retry_without = sum_scalar(all_sca_csv, 'packetSentToPeerWithoutRetry:count')
        total_retry_frames = int((retry_with or 0) + (retry_without or 0))
        print(f"ğŸ” ì¬ì‹œë„ í”„ë ˆì„: {int(retry_with or 0)}/{total_retry_frames}  ({(retry_with or 0)/(total_retry_frames or 1)*100:.2f}%)")
        print("âœ… ì™„ë£Œ")
        return

    # 2) në³„ ì²˜ë¦¬
    summary_rows = []
    for n, files in sorted(runs.items()):
        print(f"\n=== n={n} ì²˜ë¦¬ ì‹œì‘ ===")
        out = make_outputs_for_n(OUTPUT_DIR, n)

        vec_files = files.get('vec', [])
        sca_files = files.get('sca', [])
        if not vec_files or not sca_files:
            print(f"âš ï¸  n={n}: vec/sca íŒŒì¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê±´ë„ˆëœ€.")
            continue

        # ë¤í”„
        dump_all(vec_files, sca_files, out['all_vec_csv'], out['all_sca_csv'])

        # í•„í„°
        filter_vectors(out['all_vec_csv'], out['thr_csv'], THR_NAME_KEYS, THR_MODULE_KEYS)
        filter_vectors(out['all_vec_csv'], out['cw_csv'], CW_NAME_KEYS, [])
        filter_rows(out['all_sca_csv'], out['retry_csv'], name_exact=RETRY_NAMES)

        # ìŠ¤ë£¨í’‹ ê³„ì‚°
        thr_df = compute_avg_bps(out['thr_csv'], out['thr_bps_csv'])

        # ì¬ì‹œë„/ë“œë¡­ ë“± ìš”ì•½(ìŠ¤ì¹¼ë¼ì—ì„œ)
        retry_with = sum_scalar(out['all_sca_csv'], 'packetSentToPeerWithRetry:count')
        retry_without = sum_scalar(out['all_sca_csv'], 'packetSentToPeerWithoutRetry:count')
        retry_limit = sum_scalar(out['all_sca_csv'], 'retryLimitReached:count')  # ì—†ìœ¼ë©´ 0 ë°˜í™˜ ì²˜ë¦¬ë¨
        total_tx = sum_scalar(out['all_sca_csv'], 'packetSentToPeer:count') or 0

        # succ_bytesëŠ” ê¸°ì¡´ ë¡œì§/ë ˆì½”ë”ì— ë”°ë¼ ë‹¤ë¥´ë¯€ë¡œ, ì—†ìœ¼ë©´ 0
        succ_bytes = sum_scalar(out['all_sca_csv'], 'rcvdPk:sum(packetBytes)') or 0
        succ_bits = 8 * succ_bytes

        # ê°„ë‹¨ ìš”ì•½ í–‰
        summary_rows.append({
            'n': n,
            'avg_bps_from_vectors': float(thr_df['avg_bps'].mean()) if thr_df is not None and not thr_df.empty else 0.0,
            'retry_with': int(retry_with or 0),
            'retry_without': int(retry_without or 0),
            'retry_limit_drops': int(retry_limit or 0),
            'total_tx_frames': int(total_tx or 0),
            'succ_bytes': int(succ_bytes),
            'succ_bits': int(succ_bits),
        })

        # ----- Î· ê³„ì‚°(ë…¼ë¬¸ ì‹(4)) + ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥ -----
        avg_bps_mean = float(thr_df['avg_bps'].mean()) if thr_df is not None and not thr_df.empty else 0.0

        mac_metrics = compute_mac_efficiency_from_results(
            out['all_sca_csv'],
            thr_df,
            phy_rate_bps=12_000_000,   # Baseline 12 Mbps
            slot_time=9e-6,
            rifs=2e-6,
            cifs=34e-6,
            ack_time=44e-6,
            pkt_bytes=1500,
            verbose=True
        )

        # n ì „ìš© ìš”ì•½ í…ìŠ¤íŠ¸
        summary_txt_dir = out.get('out_dir') or str(Path(out['all_sca_csv']).parent)
        summary_txt_path = os.path.join(summary_txt_dir, 'packet_loss_summary.txt')
        write_packet_loss_summary_txt(
            summary_txt_path, n, mac_metrics,
            extra={
                'avg_bps': avg_bps_mean,
                'retry_with': int(retry_with or 0),
                'retry_without': int(retry_without or 0)
            }
        )
        print(f"ğŸ“ packet_loss_summary.txt ì €ì¥: {summary_txt_path}")


        # ì „ì²´ ë¹„êµ CSVì— Î·/í™•ë¥ ë„ í¬í•¨í•˜ë„ë¡ ë³„ë„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        # (main() ë§¨ ìœ„ì— eta_rows = [] ì„ ì–¸ í•„ìš”)
        eta_rows.append({
            'n': n,
            'eta': mac_metrics.get('eta', 0.0),
            'P_S': mac_metrics.get('P_S', 0.0),
            'P_I': mac_metrics.get('P_I', 0.0),
            'P_C': mac_metrics.get('P_C', 0.0),
            'avg_bps': avg_bps_mean,
            'tx_attempts': mac_metrics.get('tx_attempts', 0),
            'succ_pkts': mac_metrics.get('succ_pkts', 0),
            'collisions': mac_metrics.get('collisions', 0),
            'retry_with': int(retry_with or 0),
            'retry_without': int(retry_without or 0),
            'retry_limit_drops': int(retry_limit or 0)
        })

        print(f"=== n={n} ì²˜ë¦¬ ì™„ë£Œ ===")

    # 3) në³„ ìš”ì•½ CSV ì €ì¥
    if summary_rows:
        import pandas as pd
        summary_df = pd.DataFrame(summary_rows).sort_values('n')
        summary_path = os.path.join(OUTPUT_DIR, "summary_by_n.csv")
        summary_df.to_csv(summary_path, index=False)
        print(f"\nğŸ“Š në³„ ìš”ì•½ ì €ì¥: {summary_path}")

        # 4) në³„ MAC íš¨ìœ¨ ë¹„êµ CSV ì €ì¥
    if eta_rows:
        import pandas as pd
        eta_df = pd.DataFrame(eta_rows).sort_values('n')
        eta_csv_path = os.path.join(OUTPUT_DIR, "summary_eta_by_n.csv")
        eta_df.to_csv(eta_csv_path, index=False)
        print(f"ğŸ“Š në³„ MAC íš¨ìœ¨ ìš”ì•½ ì €ì¥: {eta_csv_path}")

    print("âœ… ì™„ë£Œ")



if __name__ == "__main__":
    main()
